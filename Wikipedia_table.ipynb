{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GLTBdbf4-pNy",
        "outputId": "c14e7d9a-33e4-4b4e-e52b-b649fc4a17cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kathmandu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Make a request to the Wikipedia page\n",
        "# url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
        "url='https://en.wikipedia.org/wiki/List_of_earthquakes_in_Nepal'\n",
        "req = requests.get(url)\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(req.content, \"html.parser\")\n",
        "\n",
        "# Find the table on the page\n",
        "table = soup.find(\"table\", class_=\"wikitable\") \n",
        "# Extract the data from the table\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        "    cols = row.find_all(\"td\")\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    data.append([ele for ele in cols if ele])\n",
        "\n",
        "# Print the extracted data\n",
        "# print(data)\n",
        "data[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the table from the Wikipedia page into a pandas DataFrame\n",
        "# url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
        "url='https://en.wikipedia.org/wiki/List_of_earthquakes_in_Nepal'\n",
        "df = pd.read_html(url)[0]\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "GVnIXy7w_Cdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Start the webdriver and load the page\n",
        "driver = webdriver.Firefox()\n",
        "driver.get(\"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\")\n",
        "\n",
        "# Scroll to the bottom of the page\n",
        "SCROLL_PAUSE_TIME = 2\n",
        "\n",
        "# Get the current height of the page\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "    # Get the new height of the page\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    # If the height has not changed, the page has stopped loading\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "\n",
        "    # Update the last height\n",
        "    last_height = new_height\n",
        "\n",
        "# Extract the HTML content of the page\n",
        "html = driver.page_source\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Find the table on the page\n",
        "table = soup.find(\"table\", class_=\"wikitable\")\n",
        "\n",
        "# Extract the data from the table\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        "    cols = row.find_all(\"td\")\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    data.append([ele for ele in cols if ele])\n",
        "\n",
        "# Close the webdriver\n",
        "driver.quit()\n",
        "\n",
        "# Print the extracted data\n",
        "print(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "0aQD3kTFB1OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Make a GET request to the API endpoint\n",
        "url = \"https://api.example.com/search?keyword=data\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check the status code of the response\n",
        "if response.status_code == 200:\n",
        "    # Extract the data from the response\n",
        "    data = response.json()\n",
        "\n",
        "    # Filter the data based on the keyword\n",
        "    filtered_data = [item for item in data if \"data\" in item[\"title\"].lower()]\n",
        "\n",
        "    # Print the filtered data\n",
        "    print(filtered_data)\n",
        "else:\n",
        "    # If the request was not successful, raise an error\n",
        "    raise Exception(\"Request failed with status code {}\".format(response.status_code))\n"
      ],
      "metadata": {
        "id": "7HDGh_5HDJB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pygoogle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRMcOc04DkpA",
        "outputId": "d2949ffb-932c-40d1-f2e0-15a5cdbaa799"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygoogle\n",
            "  Downloading pygoogle-0.6.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygoogle\n",
            "  Building wheel for pygoogle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygoogle: filename=pygoogle-0.6-py3-none-any.whl size=36077 sha256=1626791ff3b9875684d2075d77594164a20f82addca1e24b3d6a542190e477ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/90/5f/ee55566213091fe2539791cb6842399b41f47d707abf37cc5a\n",
            "Successfully built pygoogle\n",
            "Installing collected packages: pygoogle\n",
            "Successfully installed pygoogle-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pygoogle import pygoogle\n",
        "\n",
        "# Create an instance of the pygoogle class\n",
        "g = pygoogle(\"data\")\n",
        "\n",
        "# Set the number of results to return\n",
        "g.pages = 1\n",
        "\n",
        "# Run the search\n",
        "results = g.get_results()\n",
        "\n",
        "# Filter the results based on the keyword\n",
        "filtered_results = [result for result in results if \"data\" in result.title.lower()]\n",
        "\n",
        "# Print the filtered results\n",
        "for result in filtered_results:\n",
        "    print(result.title)\n",
        "    print(result.desc)\n",
        "    print(result.url)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "5YRUeiJ5DfGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}