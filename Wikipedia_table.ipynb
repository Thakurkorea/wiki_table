{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GLTBdbf4-pNy",
        "outputId": "c14e7d9a-33e4-4b4e-e52b-b649fc4a17cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kathmandu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Make a request to the Wikipedia page\n",
        "# url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
        "url='https://en.wikipedia.org/wiki/List_of_earthquakes_in_Nepal'\n",
        "req = requests.get(url)\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(req.content, \"html.parser\")\n",
        "\n",
        "# Find the table on the page\n",
        "table = soup.find(\"table\", class_=\"wikitable\") \n",
        "# Extract the data from the table\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        "    cols = row.find_all(\"td\")\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    data.append([ele for ele in cols if ele])\n",
        "\n",
        "# Print the extracted data\n",
        "# print(data)\n",
        "data[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the table from the Wikipedia page into a pandas DataFrame\n",
        "# url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
        "url='https://en.wikipedia.org/wiki/List_of_earthquakes_in_Nepal'\n",
        "df = pd.read_html(url)[0]\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "GVnIXy7w_Cdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Start the webdriver and load the page\n",
        "driver = webdriver.Firefox()\n",
        "driver.get(\"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\")\n",
        "\n",
        "# Scroll to the bottom of the page\n",
        "SCROLL_PAUSE_TIME = 2\n",
        "\n",
        "# Get the current height of the page\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "    # Get the new height of the page\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    # If the height has not changed, the page has stopped loading\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "\n",
        "    # Update the last height\n",
        "    last_height = new_height\n",
        "\n",
        "# Extract the HTML content of the page\n",
        "html = driver.page_source\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Find the table on the page\n",
        "table = soup.find(\"table\", class_=\"wikitable\")\n",
        "\n",
        "# Extract the data from the table\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        "    cols = row.find_all(\"td\")\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    data.append([ele for ele in cols if ele])\n",
        "\n",
        "# Close the webdriver\n",
        "driver.quit()\n",
        "\n",
        "# Print the extracted data\n",
        "print(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "0aQD3kTFB1OJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}